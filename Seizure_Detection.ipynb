{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R8rerG_Y0cWB",
        "outputId": "6a571767-de49-492e-b631-39f15677a39f"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VXuD95VwxIpQ",
        "outputId": "074e98d5-94f5-4a85-f9de-2470f6eec86a"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "pip install torch_geometric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mMiP2PIjvUUF",
        "outputId": "cc1931a5-5706-4f10-d143-4971c127cb31"
      },
      "outputs": [],
      "source": [
        "!pip install sklearn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HzpcF6Gm_5h7"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import numpy as np\n",
        "import os\n",
        "import random\n",
        "import sklearn\n",
        "from torch.nn import Linear, Sequential, BatchNorm1d, ReLU, Dropout\n",
        "\n",
        "from torch_geometric.nn import GCNConv, GINConv\n",
        "from torch_geometric.nn import global_mean_pool, global_add_pool\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, roc_auc_score, roc_curve, auc\n",
        "import matplotlib.pyplot as plt\n",
        "from torch_geometric.utils import to_dense_adj\n",
        "from torch_geometric.data import Data\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GCNConv, global_mean_pool\n",
        "from torch_geometric.nn import global_max_pool\n",
        "from torch.nn import Sequential, Linear, ReLU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "58MdCFOwPXdc"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kGzKwxAl7-Oy"
      },
      "source": [
        "## Seizure Data Object list:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o55GHIWW9JZu",
        "outputId": "d3ee490d-fcf3-467f-b31a-d4c65b7d984a"
      },
      "outputs": [],
      "source": [
        "\n",
        "node_dir = \"/content/drive/MyDrive/graph_data/Features/absolute/Seizure\"\n",
        "edge_dir = \"/content/drive/MyDrive/graph_data/Adjacency Matrix/Seizure\"\n",
        "seizure_data_list = []\n",
        "\n",
        "try:\n",
        "  node_files = [f for f in os.listdir(node_dir) if f.endswith(\".csv\")]\n",
        "  edge_files = [f for f in os.listdir(edge_dir) if f.endswith(\".npy\")]\n",
        "\n",
        "\n",
        "  if len(node_files) != len(edge_files):\n",
        "      raise ValueError(\"The number of node feature files and edge index files do not match.\")\n",
        "\n",
        "  for node_file in node_files:\n",
        "\n",
        "      load_path = os.path.join(node_dir, node_file)\n",
        "      node_df = pd.read_csv(load_path, header=None)\n",
        "      node_df = node_df.drop(node_df.index[0])\n",
        "      node_df = node_df.drop(node_df.columns[0], axis=1)\n",
        "      node_features = node_df.values.astype(float)\n",
        "      x = torch.tensor(node_features, dtype=torch.float)\n",
        "\n",
        "\n",
        "      edge_file = node_file.replace(\".csv\", \".npy\")\n",
        "      if edge_file not in edge_files:\n",
        "          raise FileNotFoundError(f\"Edge file for {node_file} not found.\")\n",
        "      edge_load_path = os.path.join(edge_dir, edge_file)\n",
        "\n",
        "\n",
        "      edge_index = np.load(edge_load_path)\n",
        "      adj_t = torch.tensor(edge_index, dtype=torch.long)\n",
        "\n",
        "      edge_index = adj_t.nonzero().t().contiguous()\n",
        "\n",
        "      # adjacency = to_dense_adj(edge_index)[0]\n",
        "      # adjacency += torch.eye(len(adjacency))\n",
        "      # edge_index = adjacency\n",
        "\n",
        "\n",
        "      y = torch.tensor([1], dtype=torch.long)  # label: 1 is Seizure\n",
        "      data = Data(x=x, edge_index=edge_index, y=y)\n",
        "      seizure_data_list.append(data)\n",
        "\n",
        "  print(f\"Loaded {len(seizure_data_list)} data objects.\")\n",
        "\n",
        "except Exception as e:\n",
        "  print(f\"Following Error {e} occured\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UDag_OUCew1m",
        "outputId": "793530e3-0d10-4a74-f860-bf08af07bf3b"
      },
      "outputs": [],
      "source": [
        "seizure_data_list[15].edge_index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lOrsrmP9eLU9",
        "outputId": "22ac845c-c1fc-4357-ca5e-0671b2fb3e2c"
      },
      "outputs": [],
      "source": [
        "import itertools\n",
        "edges = list(itertools.permutations(range(18),2))\n",
        "full_adj = torch.tensor(edges,dtype = torch.long).t().contiguous()\n",
        "print(full_adj)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pRwtB1_v-XLj",
        "outputId": "6bbe8362-a53c-4585-fb52-2b01b743e60f"
      },
      "outputs": [],
      "source": [
        "\n",
        "node_dir = \"/content/drive/MyDrive/graph_data/Features/absolute/NonSeizure\"\n",
        "edge_dir = \"/content/drive/MyDrive/graph_data/Adjacency Matrix/NonSeizure\"\n",
        "interictal_data_list = []\n",
        "\n",
        "try:\n",
        "  node_files = [f for f in os.listdir(node_dir) if f.endswith(\".csv\")]\n",
        "  edge_files = [f for f in os.listdir(edge_dir) if f.endswith(\".npy\")]\n",
        "\n",
        "\n",
        "  if len(node_files) != len(edge_files):\n",
        "      raise ValueError(\"The number of node feature files and edge index files do not match.\")\n",
        "\n",
        "  for node_file in node_files:\n",
        "\n",
        "      load_path = os.path.join(node_dir, node_file)\n",
        "      node_df = pd.read_csv(load_path, header=None)\n",
        "      node_df = node_df.drop(node_df.index[0])\n",
        "      node_df = node_df.drop(node_df.columns[0], axis=1)\n",
        "      node_features = node_df.values.astype(float)\n",
        "      x = torch.tensor(node_features, dtype=torch.float)\n",
        "\n",
        "\n",
        "      edge_file = node_file.replace(\".csv\", \".npy\")\n",
        "      if edge_file not in edge_files:\n",
        "          raise FileNotFoundError(f\"Edge file for {node_file} not found.\")\n",
        "      edge_load_path = os.path.join(edge_dir, edge_file)\n",
        "\n",
        "\n",
        "      edge_index = np.load(edge_load_path)\n",
        "      adj_t = torch.tensor(edge_index, dtype=torch.long)\n",
        "\n",
        "      edge_index = adj_t.nonzero().t().contiguous()\n",
        "      # adjacency = to_dense_adj(edge_index)[0]\n",
        "      # adjacency += torch.eye(len(adjacency))\n",
        "      # edge_index = adjacency\n",
        "\n",
        "      y = torch.tensor([0], dtype=torch.long)  # label: 0 is Non Seizure\n",
        "      data = Data(x=x, edge_index=edge_index, y=y)\n",
        "      interictal_data_list.append(data)\n",
        "\n",
        "  print(f\"Loaded {len(interictal_data_list)} data objects.\")\n",
        "\n",
        "except Exception as e:\n",
        "  print(f\"Following Error {e} occured\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bej2Ta9IAWHJ"
      },
      "outputs": [],
      "source": [
        "data_list = seizure_data_list + interictal_data_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qCAtJ3jiCR3O"
      },
      "outputs": [],
      "source": [
        "random.shuffle(data_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z9YiGVhPJR_m"
      },
      "outputs": [],
      "source": [
        "from torch_geometric.loader import DataLoader\n",
        "train_data = data_list[:int(len(data_list)*0.82)]\n",
        "val_data= data_list[int(len(data_list)* 0.82): int(len(data_list)* 0.9)]\n",
        "test_data = data_list[int(len(data_list)* 0.9):]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ecZOfEUM7em2",
        "outputId": "9ac96591-63e4-4745-9fdd-50b3eb1870a1"
      },
      "outputs": [],
      "source": [
        "len(train_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3AkSDX9q7hMR",
        "outputId": "0e346481-d400-47e5-a35e-c845a6b3c93b"
      },
      "outputs": [],
      "source": [
        "len(val_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SgfVOwIx7ipj",
        "outputId": "821c6ac4-f304-4dcb-c563-fbb042700753"
      },
      "outputs": [],
      "source": [
        "\n",
        "len(test_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5zqjeGk5ez6o"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gFJYsFfPHMqE"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Leaojm7uPa16",
        "outputId": "86826f79-68f9-4742-d509-30c79d83cc70"
      },
      "outputs": [],
      "source": [
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qI8dJwUCfZgb"
      },
      "source": [
        "## Graph Isomorphism Network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ys0HK8yUXPDI"
      },
      "source": [
        "###  Graph Embedding is created using concatention from the global pool of the 3 GINConv Layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iFFifgvCfZAP"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GlobalAttention\n",
        "from torch.nn import Linear\n",
        "class GIN(torch.nn.Module):\n",
        "    \"\"\"GIN\"\"\"\n",
        "    def __init__(self, dim_h,):\n",
        "        super(GIN, self).__init__()\n",
        "        self.conv1 = GINConv(\n",
        "            Sequential(Linear(num_node_features, dim_h),\n",
        "                       BatchNorm1d(dim_h), ReLU(),\n",
        "                       Linear(dim_h, dim_h), ReLU()))\n",
        "        self.conv2 = GINConv(\n",
        "            Sequential(Linear(dim_h, dim_h), BatchNorm1d(dim_h), ReLU(),\n",
        "                       Linear(dim_h, dim_h), ReLU()))\n",
        "        self.conv3 = GINConv(\n",
        "            Sequential(Linear(dim_h, dim_h), BatchNorm1d(dim_h), ReLU(),\n",
        "                       Linear(dim_h, dim_h), ReLU()))\n",
        "        #self.conv4 = GINConv(\n",
        "         #    Sequential(Linear(dim_h//4, dim_h//8), BatchNorm1d(dim_h//8), ReLU(),\n",
        "          #      Linear(dim_h//8, dim_h//8), ReLU()))\n",
        "\n",
        "\n",
        "        self.lin1 = Linear(dim_h*2,dim_h)\n",
        "        self.lin2 = Linear(dim_h, 1)\n",
        "\n",
        "    def forward(self, x, edge_index, batch):\n",
        "        import itertools\n",
        "        edges = list(itertools.permutations(range(18),2))\n",
        "        full_adj = torch.tensor(edges,dtype = torch.long).t().contiguous().to(device)\n",
        "\n",
        "        h1 = self.conv1(x, edge_index)\n",
        "        h1 = F.elu(h1)\n",
        "        h2 = self.conv2(h1, edge_index)\n",
        "\n",
        "      #  h4 = self.conv4(h3, edge_index)\n",
        "\n",
        "        m1 = global_max_pool(h1, batch)\n",
        "        a1 = global_mean_pool(h1, batch)\n",
        "        m2 = global_max_pool(h2, batch)\n",
        "        a2 = global_mean_pool(h2,batch)\n",
        "        # m3 = global_max_pool(h3, batch)\n",
        "        # a3 = global_mean_pool(h3, batch)\n",
        "\n",
        "\n",
        "        h = torch.cat((a1,a2), dim = 1)\n",
        "       # h = global_mean_pool(h2,batch)\n",
        "\n",
        "\n",
        "\n",
        "        #h3 = global_mean_pool(h3, batch)\n",
        "        # h = torch.cat9((a_pool1, a_pool2, a_pool3), dim = 1)\n",
        "\n",
        "\n",
        "        h = self.lin1(h)\n",
        "        h = F.relu(h)\n",
        "        h = F.dropout(h, p = 0.5 , training = self.training)\n",
        "        h = self.lin2(h)\n",
        "        #print(len(torch.sigmoid(h)))\n",
        "        return torch.sigmoid(h)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "177peA_cXSMk"
      },
      "source": [
        "### GRAPH EMBEDDING: Global Mean Pooling\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T89nhRigMQV5"
      },
      "outputs": [],
      "source": [
        "\n",
        "class GIN(torch.nn.Module):\n",
        "    \"\"\"GIN\"\"\"\n",
        "    def __init__(self, dim_h):\n",
        "        super(GIN, self).__init__()\n",
        "        self.conv1 = GINConv(\n",
        "            Sequential(Linear(num_node_features, dim_h),\n",
        "                       BatchNorm1d(dim_h), ReLU(),\n",
        "                       Linear(dim_h, dim_h), ReLU()))\n",
        "        self.conv2 = GINConv(\n",
        "            Sequential(Linear(dim_h, dim_h), BatchNorm1d(dim_h), ReLU(),\n",
        "                       Linear(dim_h, dim_h), ReLU()))\n",
        "        self.conv3 = GINConv(\n",
        "            Sequential(Linear(dim_h, dim_h), BatchNorm1d(dim_h), ReLU(),\n",
        "                       Linear(dim_h, dim_h), ReLU()))\n",
        "        self.lin1 = Linear(dim_h*3, dim_h*3)\n",
        "        self.lin2 = Linear(dim_h*3, 1)\n",
        "        self.lin   = Linear(dim_h, 1)\n",
        "\n",
        "    def forward(self, x, edge_index, batch):\n",
        "        h1 = self.conv1(x, edge_index)\n",
        "        h1 = F.relu(h1)\n",
        "        h2 = self.conv2(h1, edge_index)\n",
        "        h2 = F.relu(h2)\n",
        "        h3 = self.conv3(h2, edge_index)\n",
        "\n",
        "\n",
        "        hG = global_mean_pool(h3, batch)\n",
        "\n",
        "        x = F.dropout(hG,p = 0.5, training = self.training)\n",
        "        x = self.lin(x)\n",
        "\n",
        "        #print(len(torch.sigmoid(h)))\n",
        "        return torch.sigmoid(x)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-jhsYwbD678X"
      },
      "source": [
        "## Graph Convolutional Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aQBwy4pNWGFe"
      },
      "outputs": [],
      "source": [
        "class GCN(torch.nn.Module):\n",
        "  def __init__(self,dim_h):\n",
        "    super(GCN, self).__init__()\n",
        "    name = \"GCN\"\n",
        "    self.conv1 = GCNConv(num_node_features, dim_h)\n",
        "    self.conv2 = GCNConv(dim_h, dim_h//2)\n",
        "    self.conv3 = GCNConv(dim_h//2, dim_h//4)\n",
        "    #self.conv4 = GCNConv(dim_h, dim_h)\n",
        "    self.lin1 = Linear((dim_h//4+dim_h+dim_h//2), dim_h)\n",
        "    self.lin2 = Linear(dim_h, 1)\n",
        "\n",
        "\n",
        "  def forward(self, x, edge_index, batch):\n",
        "    import itertools\n",
        "    edges = list(itertools.permutations(range(18),2))\n",
        "    full_adj = torch.tensor(edges,dtype = torch.long).t().contiguous().to(device)\n",
        "    h1 = self.conv1(x, edge_index)\n",
        "    x = F.relu(h1)\n",
        "    h2 = self.conv2(x, edge_index)\n",
        "    x = F.relu(h2)\n",
        "    h3 = self.conv3(x, full_adj)\n",
        "\n",
        "    m1 = global_max_pool(h1, batch)\n",
        "    a1 = global_mean_pool(h1, batch)\n",
        "    m2 = global_max_pool(h2, batch)\n",
        "    a2 = global_mean_pool(h2,batch)\n",
        "    a3 = global_mean_pool(h3,batch)\n",
        "\n",
        "    #h = global_mean_pool(h3, batch)\n",
        "\n",
        "    h = torch.cat((a1,a2,a3), dim = 1)\n",
        "    #print(f\"The shape after concat {h.shape}\",)\n",
        "\n",
        "    h = self.lin1(h)\n",
        "    h = F.relu(h)\n",
        "    h = F.dropout(h, p = 0.5 , training = self.training)\n",
        "    h = self.lin2(h)\n",
        "    #print(len(torch.sigmoid(h)))\n",
        "    return torch.sigmoid(h)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NGBOwKR_Wlti"
      },
      "source": [
        "# GCN with Global Mean Pooling:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wjeVu0rfCRsy"
      },
      "outputs": [],
      "source": [
        "class GCN_m(torch.nn.Module):\n",
        "  def __init__(self,dim_h):\n",
        "    super(GCN_m, self).__init__()\n",
        "\n",
        "    self.conv1 = GCNConv(num_node_features, dim_h)\n",
        "    self.conv2 = GCNConv(dim_h, dim_h//2)\n",
        "    self.conv3 = GCNConv(dim_h//2, dim_h//4)\n",
        "    #self.conv4 = GCNConv(dim_h, dim_h)\n",
        "    self.lin   = Linear(dim_h//4, 1)\n",
        "\n",
        "\n",
        "\n",
        "  def forward(self,x, edge_index, batch):\n",
        "    import itertools\n",
        "    edges = list(itertools.permutations(range(18),2))\n",
        "    full_adj = torch.tensor(edges,dtype = torch.long).t().contiguous().to(device)\n",
        "    x = self.conv1(x, edge_index)\n",
        "    x = F.relu(x)\n",
        "    x = self.conv2(x,edge_index)\n",
        "    x = F.relu(x)\n",
        "    x = self.conv3(x ,full_adj)\n",
        "    #x = F.relu(x)\n",
        "   # x = self.conv4(x, edge_index)\n",
        "    #print(\"After Layer 3\")\n",
        "    #print(x.shape)\n",
        "\n",
        "    hG = global_mean_pool(x,batch)\n",
        "\n",
        "    x = F.dropout(hG,p = 0.5, training = self.training)\n",
        "    x = self.lin(x)\n",
        "    #print(len(x))\n",
        "\n",
        "    return torch.sigmoid(x)\n",
        "    #print(len(x))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1B3bSoOAU4Ci"
      },
      "source": [
        "## Graph Attention Networks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8-2W83erU3tF"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import Linear\n",
        "from torch_geometric.nn import GATv2Conv, global_mean_pool, global_max_pool\n",
        "\n",
        "class GAT(torch.nn.Module):\n",
        "    def __init__(self, dim_h, heads=8):\n",
        "        super().__init__()\n",
        "        self.gat1 = GATv2Conv(num_node_features, dim_h, heads=heads)\n",
        "        self.gat2 = GATv2Conv(dim_h * heads, dim_h, heads=1)\n",
        "\n",
        "\n",
        "        self.lin = Linear(dim_h+dim_h*heads, 1)\n",
        "\n",
        "    def forward(self, x, edge_index, batch):\n",
        "        import itertools\n",
        "        edges = list(itertools.permutations(range(18),2))\n",
        "        full_adj = torch.tensor(edges,dtype = torch.long).t().contiguous().to(device)\n",
        "\n",
        "        h = F.dropout(x, p=0.5, training=self.training)\n",
        "        h1 = self.gat1(h, edge_index)\n",
        "        h1 = F.relu(h1)\n",
        "        h1 = F.dropout(h1, p=0.5, training=self.training)\n",
        "        h2 = self.gat2(h1, edge_index)\n",
        "\n",
        "       # m1 = global_max_pool(h1, batch)\n",
        "        a1 = global_mean_pool(h1, batch)\n",
        "      # m2 = global_max_pool(h2, batch)\n",
        "        a2 = global_mean_pool(h2, batch)\n",
        "\n",
        "        #h = torch.cat((m1, a1, m2, a2), dim=1)\n",
        "        h = torch.cat((a1,a2),dim = 1)\n",
        "       # h  = global_mean_pool(h2,batch)\n",
        "\n",
        "        x = F.dropout(h, p=0.5, training=self.training)\n",
        "        x = self.lin(x)\n",
        "        return torch.sigmoid(x)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LO4Aa3MvU2FS"
      },
      "source": [
        "## GraphSAGE\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fg69ZmyyQEnn"
      },
      "outputs": [],
      "source": [
        "from binascii import a2b_hex\n",
        "\n",
        "from torch_geometric.nn import SAGEConv\n",
        "\n",
        "class GraphSAGE(torch.nn.Module):\n",
        "    def __init__(self, dim_h):\n",
        "        super().__init__()\n",
        "        self.sage1 = SAGEConv(num_node_features, dim_h)\n",
        "        self.sage2 = SAGEConv(dim_h, dim_h//2)\n",
        "        self.sage3 = SAGEConv(dim_h//2, dim_h//4)\n",
        "        self.lin = Linear((dim_h//4), 1)\n",
        "\n",
        "    def forward(self, x, edge_index, batch):\n",
        "        import itertools\n",
        "        edges = list(itertools.permutations(range(18),2))\n",
        "        full_adj = torch.tensor(edges,dtype = torch.long).t().contiguous().to(device)\n",
        "\n",
        "        h1 = self.sage1(x, edge_index)\n",
        "        h = F.elu(h1)\n",
        "        h = F.dropout(h, p=0.5, training=self.training)\n",
        "        h2 = self.sage2(h, edge_index)\n",
        "        h = F.elu(h2)\n",
        "        h = F.dropout(h, p=0.5, training=self.training)\n",
        "        h3 = self.sage3(h,full_adj)\n",
        "\n",
        "\n",
        "        a1 = global_mean_pool(h1,batch)\n",
        "        m1 = global_max_pool(h1,batch)\n",
        "        a2 = global_mean_pool(h2,batch)\n",
        "        m2 = global_max_pool(h2,batch)\n",
        "\n",
        "        #h = torch.cat((a1,a2), dim = 1)\n",
        "        h = global_mean_pool(h3, batch)\n",
        "        x = F.dropout(h, p=0.5, training=self.training)\n",
        "        x = self.lin(x)\n",
        "        return torch.sigmoid(x)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PrOLiJ00UULp"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_8CEUCVrXZ2k"
      },
      "source": [
        "### GRAPH EMBEDDING: Concatenation of the addition of the 3 GCN Layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lzVj05i-2qdW"
      },
      "outputs": [],
      "source": [
        "# class GCN(torch.nn.Module):\n",
        "#     def __init__(self, dim_h):\n",
        "#         super(GCN, self).__init__()\n",
        "#         self.conv1 = GCNConv(num_node_features, dim_h)\n",
        "#         self.conv2 = GCNConv(dim_h, dim_h)\n",
        "#         self.conv3 = GCNConv(dim_h, dim_h)\n",
        "#         self.lin1 = Linear(dim_h * 3, dim_h * 3)\n",
        "#         self.lin2 = Linear(dim_h * 3, 1)\n",
        "\n",
        "#     def forward(self, x, edge_index, batch):\n",
        "#         h1 = self.conv1(x, edge_index)\n",
        "#         h1 = F.relu(h1)\n",
        "#         h2 = self.conv2(h1, edge_index)\n",
        "#         h2 = F.relu(h2)\n",
        "#         h3 = self.conv3(h2, edge_index)\n",
        "\n",
        "#         # Apply global pooling on the output of each convolutional layer\n",
        "#         h1_pool = global_add_pool(h1, batch)\n",
        "#         h2_pool = global_add_pool(h2, batch)\n",
        "#         h3_pool = global_add_pool(h3, batch)\n",
        "\n",
        "#         # Concatenate the pooled features from each layer\n",
        "#         h = torch.cat((h1_pool, h2_pool, h3_pool), dim=1)\n",
        "\n",
        "#         # Apply linear layers and dropout\n",
        "#         h = self.lin1(h)\n",
        "#         h = h.relu()\n",
        "#         h = F.dropout(h, p=0.5, training=self.training)\n",
        "#         h = self.lin2(h)\n",
        "\n",
        "#         return torch.sigmoid(h)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "99QQFbcWfv7_"
      },
      "outputs": [],
      "source": [
        "def Roc_curve(labels, preds, model_name):\n",
        "  fpr, tpr, thresholds = roc_curve(labels, preds)\n",
        "  plt.figure(figsize = (4,4))\n",
        "  plt.plot(fpr, tpr)\n",
        "  plt.title(f\"{model_name}\")\n",
        "  plt.xlim([0.0, 1.0])\n",
        "  plt.ylim([0.0, 1.05])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DPICJmEaaP86"
      },
      "outputs": [],
      "source": [
        "def loader(batch_size):\n",
        "  train_loader = DataLoader(train_data, batch_size = batch_size, shuffle = True)\n",
        "  val_loader = DataLoader(val_data, batch_size = batch_size, shuffle = True)\n",
        "  test_loader = DataLoader(test_data, batch_size = 1 , shuffle = True)\n",
        "\n",
        "  return train_loader, val_loader, test_loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JkkznX4oxpcd"
      },
      "outputs": [],
      "source": [
        "def save_classification_report_txt(report_str, model_name, file_path):\n",
        "    with open(file_path, 'w') as f:\n",
        "        f.write(f\"Classification Report for {model_name}:\\n\")\n",
        "        f.write(report_str)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NKub2A-nbYmb"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "def Confusion_matrix(confmat, model_name):\n",
        "\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(4, 4))\n",
        "    ax.matshow(confmat, cmap=plt.cm.Blues, alpha=0.3)\n",
        "    for i in range(confmat.shape[0]):\n",
        "        for j in range(confmat.shape[1]):\n",
        "            ax.text(x=j, y=i, s=confmat[i, j], va='center', ha='center')\n",
        "\n",
        "\n",
        "    ax.set_xticklabels(['']+['Not Seizure', 'Seizure'])\n",
        "    ax.set_yticklabels(['']+['Not Seizure', 'Seizure'])\n",
        "\n",
        "    plt.xlabel('Predicted Label')\n",
        "    plt.ylabel('True Label')\n",
        "    plt.title(f\"{model_name}\")\n",
        "    # ax.xaxis.set_label_position('top')\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n",
        "def c_matplot(model, test_loader, j):\n",
        "  all_labels = []\n",
        "  all_preds = []\n",
        "  model_name = type(model).__name__\n",
        "  #model.eval()\n",
        "  #with torch.no_grad():\n",
        "  for data in test_loader:\n",
        "    data.x = data.x.to(device)\n",
        "    data.edge_index = data.edge_index.to(device)\n",
        "    data.batch = data.batch.to(device)\n",
        "    data.y = data.y.to(device)\n",
        "\n",
        "    out = model(data.x, data.edge_index, data.batch)\n",
        "    out = out.squeeze()\n",
        "    out = (out >= 0.5).int()\n",
        "    out = out.view(-1).detach().cpu().numpy()\n",
        "    all_labels.extend(data.y.cpu().numpy())\n",
        "    all_preds.extend(out)\n",
        "\n",
        "  clf_rp = classification_report(all_labels, all_preds)\n",
        "  save_classification_report_txt(clf_rp, model_name, f'/content/drive/MyDrive/graph_data/Classification_reports/exp{j}_{model_name}.txt')\n",
        "  conf = confusion_matrix(y_true = all_labels, y_pred = all_preds)\n",
        "  roc_curve = Roc_curve(all_labels, all_preds, model_name)\n",
        "  return Confusion_matrix(conf, model_name),print(clf_rp), roc_curve\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L1AsINBAbZRc"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AUrvdfIJbZPE"
      },
      "outputs": [],
      "source": [
        "# def train(model, loader, lr, epochs):\n",
        "#   criterion = torch.nn.BCELoss()\n",
        "#   optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
        "#   epochs = epochs\n",
        "#   model.train()\n",
        "\n",
        "#   train_accuracies = []\n",
        "#   val_accuracies = []\n",
        "#   for epoch in range(epochs+1):\n",
        "#     total_loss = 0\n",
        "#     acc = 0\n",
        "#     val_loss = 0\n",
        "#     val_acc = 0\n",
        "\n",
        "#     for data in loader:\n",
        "#       data.x = data.x.to(device)\n",
        "#       data.edge_index = data.edge_index.to(device)\n",
        "#       data.y = data.y.to(device).float()\n",
        "#       data.batch = data.batch.to(device)\n",
        "#       #data.y = data.y.view(-1,1)\n",
        "\n",
        "#       optimizer.zero_grad()\n",
        "#       out = model(data.x, data.edge_index, data.batch)\n",
        "#       out = out.view(-1)\n",
        "#       #print(\"Before view\", out)\n",
        "#       loss = criterion(out, data.y)\n",
        "#       total_loss += loss/ len(loader)\n",
        "#       acc += accuracy(out, data.y)/len(loader)\n",
        "#       loss.backward()\n",
        "#       optimizer.step()\n",
        "\n",
        "#       val_loss, val_acc = test(model, val_loader)\n",
        "#     val_accuracies.append(val_acc)\n",
        "\n",
        "#    # total_loss /= len(loader)\n",
        "#    # acc /= len(loader)\n",
        "\n",
        "#     train_accuracies.append(acc)\n",
        "#     if(epoch % 5 == 0):\n",
        "#       print(f'Epoch {epoch:>3} | Train Loss: {total_loss:.2f} | Train Acc: {acc*100:>.2f}% | Val Loss: {val_loss:.2f} | Val Acc: {val_acc*100:.2f}%')\n",
        "#     #print(len(val_accuracies), len(train_accuracies))\n",
        "\n",
        "#   model_name = type(model).__name__\n",
        "#   plt.figure(figsize=(10, 5))\n",
        "#   plt.plot(range(epochs + 1), train_accuracies, label='Training Accuracy')\n",
        "#   plt.plot(range(epochs + 1), val_accuracies, label='Validation Accuracy')\n",
        "#   plt.xlabel('Epochs')\n",
        "#   plt.ylabel('Accuracy')\n",
        "#   plt.legend()\n",
        "#   plt.title(f'Training vs. Validation Accuracy of {model_name}')\n",
        "#   plt.show()\n",
        "\n",
        "#   return model, train_accuracies, val_accuracies\n",
        "\n",
        "# @torch.no_grad()\n",
        "# def test(model, loader):\n",
        "#   criterion = torch.nn.BCELoss()\n",
        "#   #model.eval()\n",
        "#   loss = 0\n",
        "#   acc = 0\n",
        "\n",
        "#   for data in loader:\n",
        "#     data.x = data.x.to(device)\n",
        "#     data.edge_index = data.edge_index.to(device)\n",
        "#     data.y = data.y.to(device).float()\n",
        "#     data.batch = data.batch.to(device)\n",
        "\n",
        "#     out = model(data.x, data.edge_index, data.batch)\n",
        "#     # print(\"Output shape:\", out.shape)\n",
        "#     # print(\"Target shape:\", data.y.shape)\n",
        "\n",
        "#     out = out.view(-1)\n",
        "\n",
        "#     loss += criterion(out, data.y.float())/len(loader)\n",
        "#     acc  += accuracy(out, data.y)/len(loader)\n",
        "\n",
        "#   return loss, acc\n",
        "\n",
        "\n",
        "# def accuracy(pred_y, y):\n",
        "#   #print(\"Before Squeeze\", pred_y)\n",
        "#   pred = pred_y.squeeze()\n",
        "#   #print(\"After Squeeze\", pred)\n",
        "#   pred = (pred >= 0.5).float()\n",
        "#   #print(\"After thresholding\", pred)\n",
        "#   return (pred == y).sum().item()/len(y)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vTZ2PmOdXilw"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pjWcpj12T9XY"
      },
      "outputs": [],
      "source": [
        "def train(model, loader, lr, epochs,i):\n",
        "  criterion = torch.nn.BCELoss()\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
        "  epochs = epochs\n",
        "  model.train()\n",
        "\n",
        "  train_accuracies = []\n",
        "  val_accuracies = []\n",
        "  best_val_acc = 0\n",
        "  best_model_state = None\n",
        "  for epoch in range(epochs+1):\n",
        "    total_loss = 0\n",
        "    acc = 0\n",
        "    val_loss = 0\n",
        "    val_acc = 0\n",
        "\n",
        "    for data in loader:\n",
        "      data.x = data.x.to(device)\n",
        "      data.edge_index = data.edge_index.to(device)\n",
        "      data.y = data.y.to(device).float()\n",
        "      data.batch = data.batch.to(device)\n",
        "      #data.y = data.y.view(-1,1)\n",
        "\n",
        "      optimizer.zero_grad()\n",
        "      out = model(data.x, data.edge_index, data.batch)\n",
        "      out = out.view(-1)\n",
        "      #print(\"Before view\", out)\n",
        "      loss = criterion(out, data.y)\n",
        "      total_loss += loss/ len(loader)\n",
        "      acc += accuracy(out, data.y)/len(loader)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "    val_loss, val_acc = test(model, val_loader)\n",
        "    val_accuracies.append(val_acc )\n",
        "\n",
        "    #print(len(train_accuracies),len(val_accuracies))\n",
        "    if val_acc > best_val_acc:\n",
        "      best_val_acc = val_acc\n",
        "      best_train_acc = acc\n",
        "      best_epoch = epoch\n",
        "      best_model_state = model.state_dict()\n",
        "\n",
        "\n",
        "    train_accuracies.append(round(acc*100,2))\n",
        "   # total_loss /= len(loader)\n",
        "   # acc /= len(loader)\n",
        "\n",
        "\n",
        "    if(epoch % 5 == 0):\n",
        "      print(f'Epoch {epoch:>3} | Train Loss: {total_loss:.2f} | Train Acc: {acc*100:>.2f}% | Val Loss: {val_loss:.2f} | Val Acc: {val_acc:.2f}%')\n",
        "    #print(len(val_accuracies), len(train_accuracies))\n",
        "\n",
        "\n",
        "  model_name = type(model).__name__\n",
        "\n",
        "  torch.save(best_model_state, f'/content/drive/MyDrive/graph_data/Best_models/exp{i}_{model_name}.pth')\n",
        "  model.load_state_dict(best_model_state)\n",
        "\n",
        "  plt.figure(figsize=(8, 5))\n",
        "  plt.plot(range(epochs + 1), train_accuracies, label='Training Accuracy')\n",
        "  plt.plot(range(epochs + 1), val_accuracies, label='Validation Accuracy')\n",
        "  plt.xlabel('Epochs')\n",
        "  plt.ylabel('Accuracy')\n",
        "  plt.legend()\n",
        "  plt.title(f'Training vs. Validation Accuracy of {model_name}')\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "  #return model, best_val_acc, best_train_acc, best_epoch\n",
        "  return model, round(best_train_acc * 100, 2), round(best_val_acc * 100, 2), best_epoch\n",
        "@torch.no_grad()\n",
        "def test(model, loader):\n",
        "  criterion = torch.nn.BCELoss()\n",
        "  #model.eval()\n",
        "  loss = 0\n",
        "  acc = 0\n",
        "\n",
        "  for data in loader:\n",
        "    data.x = data.x.to(device)\n",
        "    data.edge_index = data.edge_index.to(device)\n",
        "    data.y = data.y.to(device).float()\n",
        "    data.batch = data.batch.to(device)\n",
        "\n",
        "    out = model(data.x, data.edge_index, data.batch)\n",
        "    # print(\"Output shape:\", out.shape)\n",
        "    # print(\"Target shape:\", data.y.shape)\n",
        "\n",
        "    out = out.view(-1)\n",
        "\n",
        "    loss += criterion(out, data.y.float())/len(loader)\n",
        "    acc  += accuracy(out, data.y)/len(loader)\n",
        "\n",
        "  return loss, round(acc *100, 2)\n",
        "\n",
        "\n",
        "def accuracy(pred_y, y):\n",
        "  #print(\"Before Squeeze\", pred_y)\n",
        "  pred = pred_y.squeeze()\n",
        "  #print(\"After Squeeze\", pred)\n",
        "  pred = (pred >= 0.5).float()\n",
        "  #print(\"After thresholding\", pred)\n",
        "  return (pred == y).sum().item()/len(y)\n",
        "\n",
        "# Logs:\n",
        "# the validation accuracy has been pushed to the epoch for loop, have to run the models again and check how the metrics are changing.\n",
        "# Check if the best model is the one that is being returned.\n",
        "# Perform all three experiments with the learning rate set to 0.01\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 961
        },
        "id": "-3DCm6cAWoPs",
        "outputId": "d08b2050-e2b5-41c3-b573-d20cf2c7b80e"
      },
      "outputs": [],
      "source": [
        "num_node_features = 9\n",
        "batch_size = 8\n",
        "train_loader, val_loader, test_loader = loader(batch_size = batch_size)\n",
        "gs = GraphSAGE(dim_h = 32).to(device)\n",
        "model, train_acc, val_acc, epochs= train(gs, train_loader, 0.001, 100,1)\n",
        "test_loss, test_acc = test(model, test_loader)\n",
        "\n",
        "print(f'Test Loss: {test_loss} | Test Acc: {test_acc}%')\n",
        "print(f'Training Accuracy: {train_acc}| Validation Accuracy: {val_acc}| Epoch: {epochs}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "17rnhLOde7-h"
      },
      "outputs": [],
      "source": [
        "#torch.save(model, f\"/content/drive/MyDrive/graph_data/best_graphSAGE.pth\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "-x2je8YnWoyU",
        "outputId": "2a73b290-8ad4-49f3-ad39-c32fea26e599"
      },
      "outputs": [],
      "source": [
        "c_matplot(model, test_loader, 2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eDqS9DSJb3h9"
      },
      "source": [
        "# Experiments"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dM3wk8Hwb-n0"
      },
      "source": [
        "## Experiment - 01\n",
        "\n",
        "*   Batch_size = 8\n",
        "*   Learning Rate = 0.001\n",
        "\n",
        "*   Epochs = 100\n",
        "\n",
        "*   node_features = 9\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 368
        },
        "id": "YDD3eHb1b_yP",
        "outputId": "c8de8d82-e50d-4430-98cb-c52b1fbe17f2"
      },
      "outputs": [],
      "source": [
        "batch_size = 8\n",
        "train_loader, val_loader, test_loader = loader(batch_size = batch_size)\n",
        "models = [GCN, GCN_m, GIN, GAT, GraphSAGE]\n",
        "trained_models = []\n",
        "testing_acc_1 = []\n",
        "training_acc_1 = []\n",
        "validation_acc_1 = []\n",
        "epoch_1 = []\n",
        "for model in models:\n",
        "  num_node_features = 9\n",
        "  model = model(dim_h = 32*8).to(device)\n",
        "  model, train_acc, val_acc, epochs = train(model, train_loader, 0.001, 100,1)\n",
        "  trained_models.append(model)\n",
        "  test_loss, test_acc = test(model, test_loader)\n",
        "  testing_acc_1.append(test_acc)\n",
        "  training_acc_1.append(train_acc)\n",
        "  validation_acc_1.append(val_acc)\n",
        "  epoch_1.append(epochs)\n",
        "  print(f'Test Loss: {test_loss : 2f} | Test Acc: {test_acc: 2f}%')\n",
        "\n",
        "dict = {'Train_acc': training_acc_1, 'Test_acc': testing_acc_1, 'Val_acc':validation_acc_1, 'Epoch':epoch_1}\n",
        "df = pd.DataFrame(dict)\n",
        "df.to_csv('/content/drive/MyDrive/graph_data/Metrics/exp1.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ghVm77M5X9DX"
      },
      "outputs": [],
      "source": [
        "for i in trained_models:\n",
        "  c_matplot(i, test_loader, 1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tI0f0Me3dLTz"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LQu_SPgBcoOV"
      },
      "source": [
        "## Experiment - 02\n",
        "*   Batch_size = 16\n",
        "*   Learning Rate = 0.001\n",
        "\n",
        "*   Epochs = 100\n",
        "\n",
        "*   node_features = 9\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hTXd2urhckVO"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "dzdMzPNzcpJr",
        "outputId": "04a08669-7fa4-45cd-dd94-0763c3309b83"
      },
      "outputs": [],
      "source": [
        "batch_size = 16\n",
        "train_loader, val_loader, test_loader = loader(batch_size = batch_size)\n",
        "trained_models_2 = []\n",
        "models = [GCN, GCN_m, GIN, GAT, GraphSAGE]\n",
        "testing_acc_2 = []\n",
        "training_acc_2 = []\n",
        "validation_acc_2 = []\n",
        "epoch_2 = []\n",
        "for model in models:\n",
        "  num_node_features = 9\n",
        "  model = model(dim_h = 32).to(device)\n",
        "  model, train_acc, val_acc, epochs = train(model, train_loader, 0.001, 100,2)\n",
        "  trained_models_2.append(model)\n",
        "  test_loss, test_acc = test(model, test_loader)\n",
        "  testing_acc_2.append(test_acc)\n",
        "  training_acc_2.append(train_acc)\n",
        "  validation_acc_2.append(val_acc)\n",
        "  epoch_2.append(epochs)\n",
        "  print(f'Test Loss: {test_loss : 2f} | Test Acc: {test_acc: 2f}%')\n",
        "\n",
        "dict = {'Train_acc': training_acc_2, 'Test_acc': testing_acc_2, 'Val_acc':validation_acc_2, 'Epoch': epoch_2}\n",
        "df = pd.DataFrame(dict)\n",
        "df.to_csv('/content/drive/MyDrive/graph_data/Metrics/exp2.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "UnAvGRuAckSm",
        "outputId": "6006c60f-aa7c-4f5f-cc2c-d10c9a3c9bce"
      },
      "outputs": [],
      "source": [
        "for i in trained_models_2:\n",
        "  c_matplot(i, test_loader,2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Obw8IlRmckPw"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0jL5wOnQgJzw"
      },
      "source": [
        "## Experiment- 03"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "574uqcMdckM-",
        "outputId": "af7bd25a-7460-44f5-c00a-9c05f2d33ca7"
      },
      "outputs": [],
      "source": [
        "batch_size = 32\n",
        "train_loader, val_loader, test_loader = loader(batch_size = batch_size)\n",
        "trained_models_3 = []\n",
        "models = [GCN, GCN_m, GIN, GAT, GraphSAGE]\n",
        "testing_acc_3 = []\n",
        "training_acc_3 = []\n",
        "validation_acc_3 = []\n",
        "epoch_3 = []\n",
        "for model in models:\n",
        "  num_node_features = 9\n",
        "  model = model(dim_h = 32).to(device)\n",
        "  model, train_acc, val_acc, epochs = train(model, train_loader, 0.001, 120,3)\n",
        "  trained_models_3.append(model)\n",
        "  test_loss, test_acc = test(model, test_loader)\n",
        "  testing_acc_3.append(test_acc)\n",
        "  training_acc_3.append(train_acc)\n",
        "  validation_acc_3.append(val_acc)\n",
        "  epoch_3.append(epochs)\n",
        "  print(f'Test Loss: {test_loss : 2f} | Test Acc: {test_acc: 2f}%')\n",
        "\n",
        "dict = {'Train_acc': training_acc_3, 'Test_acc': testing_acc_3, 'Val_acc':validation_acc_3, 'Epoch':epoch_3}\n",
        "df = pd.DataFrame(dict)\n",
        "df.to_csv('/content/drive/MyDrive/graph_data/Metrics/exp3.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "kdG94R6sckKo",
        "outputId": "e36c40fa-716d-450a-9e09-ef7e2491aec0"
      },
      "outputs": [],
      "source": [
        "for i in trained_models_3:\n",
        "  c_matplot(i, test_loader,3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2GZvtbOdyMyY"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kwZreXkoyMqI"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hNVcydy0yMn-"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yGXJElk_yMlk"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w_ccE8TQyMjm"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xzPkwi69JcA7"
      },
      "source": [
        "## Experiment: 4\n",
        "\n",
        "Learning rate is increased one order"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "fBHCiQEAyMgB",
        "outputId": "d0f4192d-4911-4341-d75c-2b8705690b38"
      },
      "outputs": [],
      "source": [
        "batch_size = 8\n",
        "train_loader, val_loader, test_loader = loader(batch_size = batch_size)\n",
        "models = [GCN, GCN_m, GIN, GAT, GraphSAGE]\n",
        "trained_models = []\n",
        "testing_acc_1 = []\n",
        "training_acc_1 = []\n",
        "validation_acc_1 = []\n",
        "epoch_1 = []\n",
        "for model in models:\n",
        "  num_node_features = 9\n",
        "  model = model(dim_h = 32).to(device)\n",
        "  model, train_acc, val_acc, epochs = train(model, train_loader, 0.01, 120,4)\n",
        "  trained_models.append(model)\n",
        "  test_loss, test_acc = test(model, test_loader)\n",
        "  testing_acc_1.append(test_acc)\n",
        "  training_acc_1.append(train_acc)\n",
        "  validation_acc_1.append(val_acc)\n",
        "  epoch_1.append(epochs)\n",
        "  print(f'Test Loss: {test_loss : 2f} | Test Acc: {test_acc: 2f}%')\n",
        "\n",
        "dict = {'Train_acc': training_acc_1, 'Test_acc': testing_acc_1, 'Val_acc':validation_acc_1, 'Epoch':epoch_1}\n",
        "df = pd.DataFrame(dict)\n",
        "df.to_csv('/content/drive/MyDrive/graph_data/Metrics/exp4.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "apyaRperI2ip"
      },
      "outputs": [],
      "source": [
        "for i in trained_models_:\n",
        "  c_matplot(i, test_loader,4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lRWaTbM3JoiQ"
      },
      "source": [
        "## Experiment: 5\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DHxlvpCpJpHr"
      },
      "outputs": [],
      "source": [
        "batch_size = 16\n",
        "train_loader, val_loader, test_loader = loader(batch_size = batch_size)\n",
        "trained_models_2 = []\n",
        "models = [GCN, GCN_m, GIN, GAT, GraphSAGE]\n",
        "testing_acc_2 = []\n",
        "training_acc_2 = []\n",
        "validation_acc_2 = []\n",
        "epoch_2 = []\n",
        "for model in models:\n",
        "  num_node_features = 9\n",
        "  model = model(dim_h = 32).to(device)\n",
        "  model, train_acc, val_acc, epochs = train(model, train_loader, 0.01, 120,5)\n",
        "  trained_models_2.append(model)\n",
        "  test_loss, test_acc = test(model, test_loader)\n",
        "  testing_acc_2.append(test_acc)\n",
        "  training_acc_2.append(train_acc)\n",
        "  validation_acc_2.append(val_acc)\n",
        "  epoch_2.append(epochs)\n",
        "  print(f'Test Loss: {test_loss : 2f} | Test Acc: {test_acc: 2f}%')\n",
        "\n",
        "dict = {'Train_acc': training_acc_2, 'Test_acc': testing_acc_2, 'Val_acc':validation_acc_2, 'Epoch': epoch_2}\n",
        "df = pd.DataFrame(dict)\n",
        "df.to_csv('/content/drive/MyDrive/graph_data/Metrics/exp5.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "db83gh0nJpZA"
      },
      "outputs": [],
      "source": [
        "for i in trained_models_2:\n",
        "  c_matplot(i, test_loader,5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N9bs8rZdJpyl"
      },
      "source": [
        "## Experiment:6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9eI0FGmkJqKC"
      },
      "outputs": [],
      "source": [
        "batch_size = 32\n",
        "train_loader, val_loader, test_loader = loader(batch_size = batch_size)\n",
        "trained_models_3 = []\n",
        "models = [GCN, GCN_m, GIN, GAT, GraphSAGE]\n",
        "testing_acc_3 = []\n",
        "training_acc_3 = []\n",
        "validation_acc_3 = []\n",
        "epoch_3 = []\n",
        "for model in models:\n",
        "  num_node_features = 9\n",
        "  model = model(dim_h = 32).to(device)\n",
        "  model, train_acc, val_acc, epochs = train(model, train_loader, 0.01, 200,6)\n",
        "  trained_models_3.append(model)\n",
        "  test_loss, test_acc = test(model, test_loader)\n",
        "  testing_acc_3.append(test_acc)\n",
        "  training_acc_3.append(train_acc)\n",
        "  validation_acc_3.append(val_acc)\n",
        "  epoch_3.append(epochs)\n",
        "  print(f'Test Loss: {test_loss : 2f} | Test Acc: {test_acc: 2f}%')\n",
        "\n",
        "dict = {'Train_acc': training_acc_3, 'Test_acc': testing_acc_3, 'Val_acc':validation_acc_3, 'Epoch':epoch_3}\n",
        "df = pd.DataFrame(dict)\n",
        "df.to_csv('/content/drive/MyDrive/graph_data/Metrics/exp6.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uHLdS2ZCJqYn"
      },
      "outputs": [],
      "source": [
        "for i in trained_models_3:\n",
        "  c_matplot(i, test_loader,6)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EoNxU8P3LK4r"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "26xWVrKOLK2G"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
